from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv

# â€”â€”â€” ì„¤ì • â€”â€”â€”
BASE_URL    = "https://gift.kakao.com"
MAIN_PAGE   = BASE_URL + "/brand/category/93"   # â€œë² ì´ì»¤ë¦¬/ë„ë„›/ë–¡â€ ë©”ì¸
MAX_ITEMS   = 1000

# í¬ë¡¬ ë“œë¼ì´ë²„ ì´ˆê¸°í™”
options = webdriver.ChromeOptions()
options.add_argument('--start-maximized')
driver = webdriver.Chrome(options=options)
wait   = WebDriverWait(driver, 10)

def crawl_subcategory(sub_name: str, sub_url: str):
    """
    í•œ ì†Œì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™ â†’ 'ìƒí’ˆ' íƒ­ í´ë¦­ â†’ ë¬´í•œ ìŠ¤í¬ë¡¤ â†’ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ â†’ CSV ì €ì¥
    """
    print(f"\nâ–¶ í¬ë¡¤ë§ ì‹œì‘: {sub_name} â†’ {sub_url}")
    driver.get(sub_url)
    time.sleep(2)

    # 'ìƒí’ˆ' íƒ­ í´ë¦­
    tabs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.link_tab')))
    for tab in tabs:
        # íƒ­ ë‚´ë¶€ span.txt_g ì— 'ìƒí’ˆ' í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if tab.find_element(By.CSS_SELECTOR, 'span.txt_g').text.strip() == 'ìƒí’ˆ':
            driver.execute_script("arguments[0].click();", tab)
            break
    time.sleep(2)

    # ë¬´í•œ ìŠ¤í¬ë¡¤ (ul.list_prd  ë‚´ë¶€)
        # â€”â€”â€” ë¬´í•œ ìŠ¤í¬ë¡¤ (ì—…ë°ì´íŠ¸) â€”â€”â€”
    scroll_box = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.list_prd')))
    loaded = 0
    while True:
        items = driver.find_elements(By.CSS_SELECTOR, 'ul.list_prd > li')
        # 1) ìµœëŒ€ ë„ë‹¬ í˜¹ì€ ë³€í™” ì—†ìœ¼ë©´ ë
        if len(items) >= MAX_ITEMS or len(items) == loaded:
            break
        loaded = len(items)

        # 2) í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤ (ì¼ë¶€ ë¡œë” íŠ¸ë¦¬ê±°ìš©)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)

        # 3) ë¦¬ìŠ¤íŠ¸ ë°•ìŠ¤ ë‚´ë¶€ë¡œ ìŠ¤í¬ë¡¤ (ìƒí’ˆ ë¡œë“œìš©)
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scroll_box)

        # 4) ìŠ¤í”¼ë„ˆ ì‚¬ë¼ì§ ëŒ€ê¸°
        try:
            wait.until(EC.invisibility_of_element((By.CSS_SELECTOR, 'div.global_spinner')))
        except:
            pass

        # 5) ì•ˆì • ëŒ€ê¸°
        time.sleep(2)

    print(f"  â‡’ ë¡œë”©ëœ ìƒí’ˆ ìˆ˜: {len(items)}ê°œ")


    # ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
    data = []
    for item in items[:MAX_ITEMS]:
        try:
            name_tag  = item.find_element(By.CSS_SELECTOR, 'strong.txt_prdname')
            brand_tag = item.find_element(By.CSS_SELECTOR, 'span.txt_brand')
            price_tag = item.find_element(By.CSS_SELECTOR, 'em.num_price')
            link_tag  = item.find_element(By.CSS_SELECTOR, 'a.link_info')
            img_tag   = item.find_element(By.CSS_SELECTOR, 'img.img_thumb')

            name       = name_tag.text.strip()
            brand      = brand_tag.text.strip()
            price      = price_tag.text.strip()
            href       = link_tag.get_attribute('href')
            product_url = href if href.startswith("http") else BASE_URL + href
            img_url    = img_tag.get_attribute('src')

            data.append([name, brand, price, product_url, img_url])
        except Exception as e:
            print("    â€¢ ìˆ˜ì§‘ ì˜¤ë¥˜:", e)
            continue

    # CSV ì €ì¥
    safe_name = sub_name.replace('/', '_')
    filename  = f"{safe_name}_ìƒí’ˆëª©ë¡.csv"
    with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ìƒí’ˆëª…', 'ë¸Œëœë“œ', 'ê°€ê²©', 'ìƒí’ˆURL', 'ì´ë¯¸ì§€URL'])
        writer.writerows(data)
    print(f"  âœ… {filename} ì €ì¥ ì™„ë£Œ")


# â€”â€”â€” ë©”ì¸ íë¦„ â€”â€”â€”
# 1) â€œë² ì´ì»¤ë¦¬/ë„ë„›/ë–¡â€ ë©”ì¸ í˜ì´ì§€ ë¡œë“œ
driver.get(MAIN_PAGE)
time.sleep(2)
try:
    wait.until(EC.invisibility_of_element((By.CSS_SELECTOR, 'div.global_spinner')))
except:
    pass

# 2) ì†Œì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘
subs = []
lis  = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'ul.list_ctgmain > li')))
for li in lis:
    name_attr = li.get_attribute('data-tiara-copy')
    if not name_attr:
        continue
    sub_name = name_attr.strip()
    try:
        a = li.find_element(By.CSS_SELECTOR, 'a.link_ctg')
        href = a.get_attribute('href')
        sub_url = href if href.startswith("http") else BASE_URL + href
        subs.append((sub_name, sub_url))
    except:
        continue

print(f"\nì´ {len(subs)}ê°œ ì†Œì¹´í…Œê³ ë¦¬ ë°œê²¬:")
for nm, url in subs:
    print(" â€¢", nm, "â†’", url)

# 3) ê° ì†Œì¹´í…Œê³ ë¦¬ ìˆœì°¨ í¬ë¡¤ë§
for sub_name, sub_url in subs:
    crawl_subcategory(sub_name, sub_url)

driver.quit()
print("\nğŸ‰ ëª¨ë“  ì†Œì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ!")
