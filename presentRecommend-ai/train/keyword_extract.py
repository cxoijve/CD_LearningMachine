import re
from collections import defaultdict
from keybert import KeyBERT
from konlpy.tag import Okt
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertForSequenceClassification
import torch

# KoBERT ëª¨ë¸ ë¡œë“œ (ë¡œì»¬ íŒŒì¸íŠœë‹ ëª¨ë¸)
model_name = "skt/kobert-base-v1"
tokenizer = KoBERTTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.load_state_dict(torch.load("../kobert_importance.pth", map_location="cpu"))
model.eval()

kw_model = KeyBERT(model="distiluse-base-multilingual-cased-v1")
okt = Okt()

# ë¶ˆìš©ì–´ ì‚¬ì „ ì¤€ë¹„
stopwords_path = "../stopwords-ko.txt"
with open(stopwords_path, encoding="utf-8") as f:
    stopwords = set(line.strip() for line in f if line.strip())

date_patterns = [
    r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼",  # ex: 2025ë…„ 5ì›” 13ì¼
]

def extract_date_key(text):
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            y, m, d = match.groups()
            return f"{int(y):04d}-{int(m):02d}-{int(d):02d}"
    return None

def is_valid_keyword(kw):
    return not re.search(r"(ë‹¤|ì–´|ì§€|ìŒ)$", kw)

def extract_kakao_dialogues(path):
    with open(path, encoding="utf-8") as f:
        lines = f.readlines()
    data_by_date = defaultdict(list)
    for line in lines:
        date_key = extract_date_key(line)
        if date_key:
            current_date = date_key
        elif re.search(r"[ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:", line):
            msg = re.sub(r"^\d{4}\. \d{1,2}\. \d{1,2}\. [ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:\s*", "", line).strip()
            if len(msg) > 0:
                data_by_date[current_date].append(msg)
    return data_by_date

# def extract_sentences_from_kakao_txt(file_path):
#     with open(file_path, encoding="utf-8") as f:
#         lines = f.readlines()
#     dialogue_lines = [
#         re.sub(r"\d{4}\. \d{1,2}\. \d{1,2}\. [ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:\s*", "", line).strip()
#         for line in lines
#         if re.search(r"\d{4}\. \d{1,2}\. \d{1,2}\.", line) is None and ":" in line
#     ]
#     return [line for line in dialogue_lines if len(line) > 1]

# ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ í•„í„°ë§
def is_valid_conversation(msg):
    if not re.search(r"[ê°€-í£]", msg):
        return False
    if re.search(r"https?://|ì´\s*ê¸ˆì•¡", msg):
        return False
    return True

# # ë¶ˆìš©ì–´ ì œê±° ë° í‚¤ì›Œë“œ ì¶”ì¶œ
# def extract_keywords(sentences):
#     keyword_scores = defaultdict(float)
#     for sentence in sentences:
#         okt_nouns = set(okt.nouns(sentence))
#         filtered_nouns = {n for n in okt_nouns if n not in stopwords and len(n) > 1}
#
#         keybert_keywords = kw_model.extract_keywords(
#             sentence, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=5
#         )
#
#         for kw, score in keybert_keywords:
#             tokens = kw.split()
#             if all(token in filtered_nouns for token in tokens) and is_valid_keyword(kw):
#                 multiplier = 2.5 if len(tokens) > 1 else 2.0
#                 keyword_scores[kw] += score * multiplier
#
#         for noun in filtered_nouns:
#             keyword_scores[noun] += 0.2
#
#     for parent_kw in list(keyword_scores.keys()):
#         for sub_kw in keyword_scores:
#             if sub_kw != parent_kw and sub_kw in parent_kw:
#                 keyword_scores[parent_kw] += 0.5 * keyword_scores[sub_kw]
#
#     return sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)

def classify_interest(sentence):
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        return int(torch.argmax(probs, dim=1))  # 1 = ê´€ì‹¬, 0 = ë¬´ê´€ì‹¬

def extract_interest_weighted_keywords(sentences):
    keyword_scores = defaultdict(float)
    for sentence in sentences:
        label = classify_interest(sentence)  # 0 or 1
        nouns = {n for n in okt.nouns(sentence) if n not in stopwords and len(n) > 1}
        keywords = kw_model.extract_keywords(sentence, keyphrase_ngram_range=(1, 2), stop_words=None, top_n=5)

        if label == 1:
            print(f"\nğŸ’¬ ë¬¸ì¥: {sentence}")
            print(f"ê´€ì‹¬ë„ ë¶„ë¥˜: ê´€ì‹¬ ìˆìŒ (1)")

        for kw, score in keywords:
            tokens = kw.split()
            if all(token in nouns for token in tokens):
                multiplier = 2.5 if len(tokens) > 1 else 2.0
                final_score = score * (multiplier if label == 1 else 0.5)
                keyword_scores[kw] += final_score
                print(f"í‚¤ì›Œë“œ: {kw:15} | base: {score:.2f} â†’ ì ìš© ì ìˆ˜: {final_score:.2f}")

        for noun in nouns:
            add_score = 0.3 if label == 1 else 0.1
            keyword_scores[noun] += add_score
            print(f"ëª…ì‚¬ ê°€ì¤‘ì¹˜: {noun:15} â†’ {add_score:.2f}")

    filtered_keywords = [
        (kw, sc) for kw, sc in keyword_scores.items()
        if all(not re.search(r"(ë‹¤|ì–´|ì§€|ìŒ|ì•¼)$", token) for token in kw.split())
    ]
    return sorted(filtered_keywords, key=lambda x: x[1], reverse=True)

file_path = "Talk_2025.5.13 16_38-1.txt"
data_by_date = extract_kakao_dialogues(file_path)

print(f"ë‚ ì§œ ë¸”ë¡ ìˆ˜: {len(data_by_date)}\n")

for date, messages in sorted(data_by_date.items()):
    filtered_msgs = [msg for msg in messages if is_valid_conversation(msg)]
    print(f"â–¶ {date} / ëŒ€í™” ìˆ˜: {len(messages)}")
    for m in messages:
        if not is_valid_conversation(m):
            print(f"ì œì™¸ëœ ë¬¸ì¥: {m}")
    print(f"ìœ íš¨ ëŒ€í™” ìˆ˜: {len(filtered_msgs)}")

    if len(filtered_msgs) == 0:
        print("ê±´ë„ˆëœ€: ì˜ë¯¸ ìˆëŠ” ëŒ€í™” ì—†ìŒ\n")
        continue

    keywords = extract_interest_weighted_keywords(filtered_msgs)
    print(f"\nğŸ“… {date} í‚¤ì›Œë“œ:")
    for kw, score in keywords[:10]:
        print(f"- {kw}: {score:.2f}")
    print()
