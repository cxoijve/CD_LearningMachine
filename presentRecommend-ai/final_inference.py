import os
import re
import torch
import pandas as pd
from collections import defaultdict
from keybert import KeyBERT
from konlpy.tag import Okt
from kobert_tokenizer import KoBERTTokenizer
from transformers import BertForSequenceClassification, AutoTokenizer, BertModel
from sentence_transformers import SentenceTransformer, util
import torch.nn as nn

# ëª¨ë¸ ë¡œë“œ
# ê´€ì‹¬(ì„ í˜¸) ë¶„ë¥˜ ëª¨ë¸
model_name = "skt/kobert-base-v1"
tokenizer = KoBERTTokenizer.from_pretrained(model_name)
interest_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
interest_model.load_state_dict(torch.load("./kobert_importance.pth", map_location="cpu"))
interest_model.eval()

# í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
kw_model = KeyBERT(model="distiluse-base-multilingual-cased-v1")
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
okt = Okt()

# ì£¼ì œ/ì¹œë°€ë„ ëª¨ë¸ ì •ì˜
class KoBertExtendedModel(nn.Module):
    def __init__(self, model_name="skt/kobert-base-v1", num_subjects=20):
        super().__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.score_head = nn.Linear(768, 1)
        self.awkward_head = nn.Linear(768, 2)
        self.subject_head = nn.Linear(768, num_subjects)

    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        score = self.score_head(pooled_output)
        awkward = self.awkward_head(pooled_output)
        subject = self.subject_head(pooled_output)
        return score, awkward, subject

topic_tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1", use_fast=False)
topic_model = KoBertExtendedModel()
topic_model.load_state_dict(torch.load("kobert_extended_with_subject.pth", map_location="cpu"), strict=False)
topic_model.eval()

# ë§¤í•‘/ìƒìˆ˜
subject_id2name = {0:"ë¯¸ìš©",1:"ìŠ¤í¬ì¸ /ë ˆì €",2:"êµìœ¡",3:"ê°€ì¡±",5:"ì˜í™”/ë§Œí™”",6:"êµí†µ",7:"ì—¬í–‰",
                   8:"íšŒì‚¬/ì•„ë¥´ë°”ì´íŠ¸",9:"ê±´ê°•",10:"ì—°ì• /ê²°í˜¼",11:"ê²Œì„",12:"ê³„ì ˆ/ë‚ ì”¨",13:"ë°©ì†¡/ì—°ì˜ˆ",
                   14:"ì‚¬íšŒì´ìŠˆ",15:"ì£¼ê±°ì™€ ìƒí™œ",16:"ë°˜ë ¤ë™ë¬¼",17:"êµ°ëŒ€",18:"ì‹ìŒë£Œ"}

subject_to_main_category = {0:"ë·°í‹°",1:"ë ˆì €/ìŠ¤í¬ì¸ ",2:"ë¦¬ë¹™/ë„ì„œ",3:"ë””ì§€í„¸/ê°€ì „",5:"íŒ¨ì…˜",
                            6:"ë””ì§€í„¸/ê°€ì „",7:"ë ˆì €/ìŠ¤í¬ì¸ ",8:"ë¦¬ë¹™/ë„ì„œ",9:"ì‹í’ˆ",10:"íŒ¨ì…˜",
                            11:"ë””ì§€í„¸/ê°€ì „",12:"ì‹í’ˆ",13:"íŒ¨ì…˜",14:"ë¦¬ë¹™/ë„ì„œ",
                            15:"ë¦¬ë¹™/ë„ì„œ",16:"ìœ ì•„ë™/ë°˜ë ¤",17:"ì‹í’ˆ",18:"ì‹í’ˆ"}

category_to_file = {
    "ë·°í‹°": "category_files/beauty.csv",
    "ë ˆì €/ìŠ¤í¬ì¸ ": "category_files/sport.csv",
    "ë¦¬ë¹™/ë„ì„œ": "category_files/living.csv",
    "ë””ì§€í„¸/ê°€ì „": "category_files/digital.csv",
    "íŒ¨ì…˜": "category_files/fashion.csv",
    "ì‹í’ˆ": "category_files/food.csv",
    "ìœ ì•„ë™/ë°˜ë ¤": "category_files/baby.csv"
}

os.makedirs("cached_embeddings", exist_ok=True)   # ìºì‹œ í´ë”

# ê¸°ëŠ¥ í•¨ìˆ˜
with open("stopwords-ko.txt", encoding="utf-8") as f:
    stopwords = set(line.strip() for line in f if line.strip())

# ìƒí’ˆ ì„ë² ë”© ìºì‹±
def load_or_build_embeddings(category: str, intimacy_score: float):
    csv_path = category_to_file.get(category)
    if not csv_path:
        return []
    if intimacy_score < 2:
        suffix = "_2"
    elif intimacy_score < 3:
        suffix = "_3"
    elif intimacy_score < 4:
        suffix = "_4"
    else:
        suffix = "_5"

    base_name = os.path.splitext(os.path.basename(csv_path))[0]
    cache_path = f"cached_embeddings/{base_name}{suffix}.pt"

    if os.path.exists(cache_path):
        return torch.load(cache_path)

    # ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ìºì‹œë¡œ fallback
    fallback_path = f"cached_embeddings/{base_name}.pt"
    if os.path.exists(fallback_path):
        print(f"[!] {cache_path} ì—†ìŒ â†’ {fallback_path} ì‚¬ìš©")
        return torch.load(fallback_path)

    print(f"[!] ìºì‹œ ì—†ìŒ: {cache_path} / {fallback_path}")
    return []

# ê´€ì‹¬ ë¶„ë¥˜ ë°°ì¹˜
def classify_interest_batch(sentences):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ â†’ [0/1, 0/1, ...] ë°˜í™˜
    """
    inputs = tokenizer(
        sentences,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )
    with torch.no_grad():
        logits = interest_model(**inputs).logits
        labels = torch.argmax(torch.softmax(logits, dim=1), dim=1)
    return labels.tolist()


def classify_topic_and_score(sentence):
    inputs = topic_tokenizer(sentence, return_tensors="pt", padding=True,
                             truncation=True, max_length=512)
    with torch.no_grad():
        score, _, subject_logits = topic_model(inputs["input_ids"], inputs["attention_mask"])

        # í´ë¦¬í•‘ í›„ ì •ê·œí™”
        score_raw = score.item()
        score_clipped = max(min(score_raw, 2.0), -2.0)  # [-2, 2]
        score_value = (score_clipped + 2.0) / 4.0 * 8  # â†’ [0, 5]

        subject_id = torch.argmax(subject_logits, dim=1).item()

    return (
        subject_id2name.get(subject_id, "ì•Œ ìˆ˜ ì—†ìŒ"),
        subject_to_main_category.get(subject_id, "ì—†ìŒ"),
        round(score_value, 2)
    )

def extract_kakao_dialogues(path):
    with open(path, encoding="utf-8") as f:
        lines = f.readlines()
    data_by_date = defaultdict(list)
    for line in lines:
        if m := re.search(r"(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼", line):
            y, m_, d = m.groups()
            current_date = f"{int(y):04d}-{int(m_):02d}-{int(d):02d}"
        elif re.search(r"[ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:", line):
            msg = re.sub(r"^\d{4}\. \d{1,2}\. \d{1,2}\. [ì˜¤ì „|ì˜¤í›„]+\s*\d{1,2}:\d{2},\s*[^:]+:\s*", "", line).strip()
            if msg:
                data_by_date[current_date].append(msg)
    return data_by_date

def is_valid_conversation(msg):
    return bool(re.search(r"[ê°€-í£]", msg)) and not re.search(r"https?://|ì´\s*ê¸ˆì•¡", msg)

def classify_avg_score_from_pairs(messages):
    """
    ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ (A, B) ë¬¸ì¥ ìŒì„ ë§Œë“¤ê³ , ê° ìŒì— ëŒ€í•´ ì¹œë°€ë„ ì˜ˆì¸¡ â†’ í‰ê·  ë°˜í™˜
    """
    if len(messages) < 2:
        return 0.0  # ë¬¸ì¥ì´ í•œ ê°œ ì´í•˜ì¼ ê²½ìš°

    pairs = [(messages[i], messages[i + 1]) for i in range(len(messages) - 1)]
    scores = []

    for a, b in pairs:
        input_text = a.strip() + " [SEP] " + b.strip()
        inputs = topic_tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            score, _, _ = topic_model(inputs["input_ids"], inputs["attention_mask"])
            score_value = torch.sigmoid(score).item() * 8  # 0~5 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            scores.append(score_value)

    return round(sum(scores) / len(scores), 2) if scores else 0.0

def classify_topic(sentence):
    inputs = topic_tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        _, _, subject_logits = topic_model(inputs["input_ids"], inputs["attention_mask"])
        subject_id = torch.argmax(subject_logits, dim=1).item()
    return subject_id2name.get(subject_id, "ì•Œ ìˆ˜ ì—†ìŒ"), subject_to_main_category.get(subject_id, "ì—†ìŒ")

def extract_interest_weighted_keywords(sentences):
    keyword_scores = defaultdict(float)
    labels = classify_interest_batch(sentences)
    for sentence, label in zip(sentences, labels):
        nouns = {n for n in okt.nouns(sentence) if n not in stopwords and len(n) > 1}
        for kw, score in kw_model.extract_keywords(sentence, (1, 2), None, top_n=5):
            if all(tok in nouns for tok in kw.split()):
                multiplier = 2.5 if " " in kw else 2.0
                keyword_scores[kw] += score * (multiplier if label == 1 else 0.5)
        for noun in nouns:
            keyword_scores[noun] += 0.3 if label == 1 else 0.1

    filtered = [(k, v) for k, v in keyword_scores.items() if all(not re.search(r"(ë‹¤|ì–´|ì§€|ìŒ)$", t) for t in k.split())]
    return sorted(filtered, key=lambda x: x[1], reverse=True)

def recommend_products_from_keywords(sorted_keywords, allowed_category, intimacy_score):
    products = load_or_build_embeddings(allowed_category, intimacy_score)
    if not products:
        return []

    query = " ".join([kw for kw, _ in sorted_keywords[:5]])
    q_emb = embedding_model.encode(query, convert_to_tensor=True)
    scores = [(p["name"], util.cos_sim(q_emb, p["embedding"]).item()) for p in products]
    return sorted(scores, key=lambda x: x[1], reverse=True)

if __name__ == "__main__":
    file_path = "chat_exam.txt"
    data_by_date = extract_kakao_dialogues(file_path)

    for date, msgs in sorted(data_by_date.items()):
        msgs = [m for m in msgs if is_valid_conversation(m)]
        if not msgs:
            continue

        full_text = " ".join(msgs)
        subject, main_cat = classify_topic(full_text)
        intimacy = classify_avg_score_from_pairs(msgs)
        keywords = extract_interest_weighted_keywords(msgs)

        print(f"\nğŸ“… {date}  ì£¼ì œ:{subject}  ëŒ€ë¶„ë¥˜:{main_cat}  ì¹œë°€ë„:{intimacy}")
        for kw, sc in keywords[:5]:
            print(f" - {kw}: {sc:.2f}")

        print("\nğŸ ì¶”ì²œ TOP 5:")
        for name, sc in recommend_products_from_keywords(
                keywords,  # sorted_keywords
                main_cat,  # allowed_category
                intimacy  # intimacy_score
        )[:5]:
            print(f" - {name} ({sc:.2f})")


